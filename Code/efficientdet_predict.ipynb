{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "!pip install --no-deps '../input/timm-package/timm-0.1.26-py3-none-any.whl' > /dev/null\n",
    "!pip install --no-deps '../input/pycocotools/pycocotools-2.0-cp37-cp37m-linux_x86_64.whl' > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../input/timm-efficientdet-pytorch\")\n",
    "sys.path.insert(0, \"../input/omegaconf\")\n",
    "sys.path.insert(0, \"../input/weightedboxesfusion\")\n",
    "\n",
    "import ensemble_boxes\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import cv2\n",
    "import gc\n",
    "from matplotlib import pyplot as plt\n",
    "from effdet import get_efficientdet_config, EfficientDet, DetBenchEval\n",
    "from effdet.efficientdet import HeadNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_transforms():\n",
    "    return A.Compose([\n",
    "            A.Resize(height=512, width=512, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here is to get test data loaded. You can add these codes to your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT_PATH = '../input/global-wheat-detection/test'\n",
    "\n",
    "class DatasetRetriever(Dataset):\n",
    "\n",
    "    def __init__(self, image_ids, transforms=None):\n",
    "        super().__init__()\n",
    "        self.image_ids = image_ids\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        image_id = self.image_ids[index]\n",
    "        image = cv2.imread(f'{DATA_ROOT_PATH}/{image_id}.jpg', cv2.IMREAD_COLOR)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "        if self.transforms:\n",
    "            sample = {'image': image}\n",
    "            sample = self.transforms(**sample)\n",
    "            image = sample['image']\n",
    "        return image, image_id\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.image_ids.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetRetriever(\n",
    "    image_ids=np.array([path.split('/')[-1][:-4] for path in glob(f'{DATA_ROOT_PATH}/*.jpg')]),\n",
    "    transforms=get_valid_transforms()\n",
    ")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    drop_last=False,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_net(checkpoint_path):\n",
    "    config = get_efficientdet_config('tf_efficientdet_d5')\n",
    "    net = EfficientDet(config, pretrained_backbone=False)\n",
    "\n",
    "    config.num_classes = 1\n",
    "    config.image_size=512\n",
    "    net.class_net = HeadNet(config, num_outputs=config.num_classes, norm_kwargs=dict(eps=.001, momentum=.01))\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    net.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    del checkpoint\n",
    "    gc.collect()\n",
    "\n",
    "    net = DetBenchEval(net, config)\n",
    "    net.eval();\n",
    "    return net.cuda()\n",
    "\n",
    "net = load_net('../input/wheat-effdet5-fold0-best-checkpoint/fold0-best-all-states.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom TTA API\n",
    "\n",
    "Idea is simple: \n",
    "- `augment` make tta for one image\n",
    "- `batch_augment` make tta for batch of images\n",
    "- `deaugment_boxes` return tta predicted boxes in back to original state of image\n",
    "\n",
    "Also we are interested in `Compose` with combinations of tta :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseWheatTTA:\n",
    "    \"\"\" author: @shonenkov \"\"\"\n",
    "    image_size = 512\n",
    "\n",
    "    def augment(self, image):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def batch_augment(self, images):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def deaugment_boxes(self, boxes):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class TTAHorizontalFlip(BaseWheatTTA):\n",
    "    \"\"\" author: @shonenkov \"\"\"\n",
    "\n",
    "    def augment(self, image):\n",
    "        return image.flip(1)\n",
    "    \n",
    "    def batch_augment(self, images):\n",
    "        return images.flip(2)\n",
    "    \n",
    "    def deaugment_boxes(self, boxes):\n",
    "        boxes[:, [1,3]] = self.image_size - boxes[:, [3,1]]\n",
    "        return boxes\n",
    "\n",
    "class TTAVerticalFlip(BaseWheatTTA):\n",
    "    \"\"\" author: @shonenkov \"\"\"\n",
    "    \n",
    "    def augment(self, image):\n",
    "        return image.flip(2)\n",
    "    \n",
    "    def batch_augment(self, images):\n",
    "        return images.flip(3)\n",
    "    \n",
    "    def deaugment_boxes(self, boxes):\n",
    "        boxes[:, [0,2]] = self.image_size - boxes[:, [2,0]]\n",
    "        return boxes\n",
    "    \n",
    "class TTARotate90(BaseWheatTTA):\n",
    "    \"\"\" author: @shonenkov \"\"\"\n",
    "    \n",
    "    def augment(self, image):\n",
    "        return torch.rot90(image, 1, (1, 2))\n",
    "\n",
    "    def batch_augment(self, images):\n",
    "        return torch.rot90(images, 1, (2, 3))\n",
    "    \n",
    "    def deaugment_boxes(self, boxes):\n",
    "        res_boxes = boxes.copy()\n",
    "        res_boxes[:, [0,2]] = self.image_size - boxes[:, [1,3]]\n",
    "        res_boxes[:, [1,3]] = boxes[:, [2,0]]\n",
    "        return res_boxes\n",
    "\n",
    "class TTACompose(BaseWheatTTA):\n",
    "    \"\"\" author: @shonenkov \"\"\"\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def augment(self, image):\n",
    "        for transform in self.transforms:\n",
    "            image = transform.augment(image)\n",
    "        return image\n",
    "    \n",
    "    def batch_augment(self, images):\n",
    "        for transform in self.transforms:\n",
    "            images = transform.batch_augment(images)\n",
    "        return images\n",
    "    \n",
    "    def prepare_boxes(self, boxes):\n",
    "        result_boxes = boxes.copy()\n",
    "        result_boxes[:,0] = np.min(boxes[:, [0,2]], axis=1)\n",
    "        result_boxes[:,2] = np.max(boxes[:, [0,2]], axis=1)\n",
    "        result_boxes[:,1] = np.min(boxes[:, [1,3]], axis=1)\n",
    "        result_boxes[:,3] = np.max(boxes[:, [1,3]], axis=1)\n",
    "        return result_boxes\n",
    "    \n",
    "    def deaugment_boxes(self, boxes):\n",
    "        for transform in self.transforms[::-1]:\n",
    "            boxes = transform.deaugment_boxes(boxes)\n",
    "        return self.prepare_boxes(boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "def process_det(index, det, score_threshold=0.37):\n",
    "    boxes = det[index].detach().cpu().numpy()[:,:4]    \n",
    "    scores = det[index].detach().cpu().numpy()[:,4]\n",
    "    boxes[:, 2] = boxes[:, 2] + boxes[:, 0]\n",
    "    boxes[:, 3] = boxes[:, 3] + boxes[:, 1]\n",
    "    boxes = (boxes).clip(min=0, max=511).astype(int)\n",
    "    indexes = np.where(scores>score_threshold)\n",
    "    boxes = boxes[indexes]\n",
    "    scores = scores[indexes]\n",
    "    return boxes, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combinations of TTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "tta_transforms = []\n",
    "for tta_combination in product([TTAHorizontalFlip(), None], \n",
    "                               [TTAVerticalFlip(), None],\n",
    "                               [TTARotate90(), None]):\n",
    "    tta_transforms.append(TTACompose([tta_transform for tta_transform in tta_combination if tta_transform]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WBF over TTA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tta_predictions(images, score_threshold=0.25):\n",
    "    with torch.no_grad():\n",
    "        images = torch.stack(images).float().cuda()\n",
    "        predictions = []\n",
    "        for tta_transform in tta_transforms:\n",
    "            result = []\n",
    "            det = net(tta_transform.batch_augment(images.clone()), torch.tensor([1]*images.shape[0]).float().cuda())\n",
    "\n",
    "            for i in range(images.shape[0]):\n",
    "                boxes = det[i].detach().cpu().numpy()[:,:4]    \n",
    "                scores = det[i].detach().cpu().numpy()[:,4]\n",
    "                indexes = np.where(scores > score_threshold)[0]\n",
    "                boxes = boxes[indexes]\n",
    "                boxes[:, 2] = boxes[:, 2] + boxes[:, 0]\n",
    "                boxes[:, 3] = boxes[:, 3] + boxes[:, 1]\n",
    "                boxes = tta_transform.deaugment_boxes(boxes.copy())\n",
    "                result.append({\n",
    "                    'boxes': boxes,\n",
    "                    'scores': scores[indexes],\n",
    "                })\n",
    "            predictions.append(result)\n",
    "    return predictions\n",
    "\n",
    "def run_wbf(predictions, image_index, image_size=512, iou_thr=0.48, skip_box_thr=0.43, weights=None):\n",
    "    boxes = [(prediction[image_index]['boxes']/(image_size-1)).tolist() for prediction in predictions]\n",
    "    scores = [prediction[image_index]['scores'].tolist() for prediction in predictions]\n",
    "    labels = [np.ones(prediction[image_index]['scores'].shape[0]).astype(int).tolist() for prediction in predictions]\n",
    "    boxes, scores, labels = ensemble_boxes.ensemble_boxes_wbf.weighted_boxes_fusion(boxes, scores, labels, weights=None, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n",
    "    boxes = boxes*(image_size-1)\n",
    "    return boxes, scores, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prediction_string(boxes, scores):\n",
    "    pred_strings = []\n",
    "    for j in zip(scores, boxes):\n",
    "        pred_strings.append(\"{0:.4f} {1} {2} {3} {4}\".format(j[0], j[1][0], j[1][1], j[1][2], j[1][3]))\n",
    "    return \" \".join(pred_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for images, image_ids in data_loader:\n",
    "    predictions = make_tta_predictions(images)\n",
    "    for i, image in enumerate(images):\n",
    "        boxes, scores, labels = run_wbf(predictions, image_index=i)\n",
    "        boxes = (boxes*2).round().astype(np.int32).clip(min=0, max=1023)\n",
    "        image_id = image_ids[i]\n",
    "\n",
    "        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n",
    "        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n",
    "\n",
    "        result = {\n",
    "            'image_id': image_id,\n",
    "            'PredictionString': format_prediction_string(boxes, scores)\n",
    "        }\n",
    "        results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>PredictionString</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>796707dd7</td>\n",
       "      <td>0.8999 709 823 111 104 0.8723 894 332 113 93 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cc3532ff6</td>\n",
       "      <td>0.9600 772 831 163 160 0.9266 910 124 112 95 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51f1be19e</td>\n",
       "      <td>0.8007 842 268 131 202 0.7976 612 85 151 169 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>51b3e36ab</td>\n",
       "      <td>0.8767 874 291 149 137 0.8746 499 361 311 126 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f5a1f0358</td>\n",
       "      <td>0.9083 689 204 113 92 0.8983 542 272 112 114 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    image_id                                   PredictionString\n",
       "0  796707dd7  0.8999 709 823 111 104 0.8723 894 332 113 93 0...\n",
       "1  cc3532ff6  0.9600 772 831 163 160 0.9266 910 124 112 95 0...\n",
       "2  51f1be19e  0.8007 842 268 131 202 0.7976 612 85 151 169 0...\n",
       "3  51b3e36ab  0.8767 874 291 149 137 0.8746 499 361 311 126 ...\n",
       "4  f5a1f0358  0.9083 689 204 113 92 0.8983 542 272 112 114 0..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.DataFrame(results, columns=['image_id', 'PredictionString'])\n",
    "test_df.to_csv('submission.csv', index=False)\n",
    "test_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
